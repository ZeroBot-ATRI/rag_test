"""
RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) ç³»ç»Ÿ - æ··åˆæœç´¢+é‡æ’ç‰ˆæœ¬
åŠŸèƒ½ï¼š
1. æ··åˆæœç´¢ï¼šç¨ å¯†å‘é‡ + BM25ç¨€ç–å‘é‡ï¼Œè·å–40ä¸ªå€™é€‰ç»“æœ
2. é‡æ’åºï¼šå¯¹40ä¸ªå€™é€‰ç»“æœè¿›è¡Œé‡æ’ï¼Œå¾—åˆ°æœ€ç»ˆ5ä¸ªç»“æœ
3. ä½¿ç”¨LLMç”Ÿæˆé«˜è´¨é‡å›ç­”
4. æä¾›å®Œæ•´çš„é—®ç­”ä½“éªŒ
"""

from sentence_transformers import SentenceTransformer
from FlagEmbedding import FlagReranker
from milvus_client import MyMilvusClient
from openai import OpenAI
import jieba
import config
import sys

class PromptTemplate:
    """æç¤ºè¯æ¨¡æ¿ç®¡ç†å™¨"""
    
    # ç³»ç»Ÿæç¤ºè¯
    SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹å­¦ä¹ åŠ©æ‰‹ï¼Œä¸“æ³¨äºPythonå’ŒJAVAæŠ€æœ¯é—®ç­”ã€‚
ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®æä¾›çš„çŸ¥è¯†åº“å†…å®¹ï¼Œä¸ºç”¨æˆ·æä¾›å‡†ç¡®ã€è¯¦ç»†ã€æ˜“æ‡‚çš„æŠ€æœ¯è§£ç­”ã€‚

å›ç­”è¦æ±‚ï¼š
1. åŸºäºæä¾›çš„å‚è€ƒèµ„æ–™å›ç­”é—®é¢˜ï¼Œä¿æŒå‡†ç¡®æ€§
2. å¦‚æœå‚è€ƒèµ„æ–™ä¸­æœ‰ä»£ç ç¤ºä¾‹ï¼Œè¯·å®Œæ•´å±•ç¤º
3. ç”¨æ¸…æ™°çš„è¯­è¨€è§£é‡ŠæŠ€æœ¯æ¦‚å¿µ
4. å¦‚æœå‚è€ƒèµ„æ–™ä¸å¤Ÿå……åˆ†ï¼Œå¯ä»¥é€‚å½“è¡¥å……ä½ çš„çŸ¥è¯†ï¼Œä½†è¦æ˜ç¡®è¯´æ˜
5. ä¿æŒä¸“ä¸šã€å‹å¥½çš„è¯­æ°”
6. ä½¿ç”¨Markdownæ ¼å¼ç»„ç»‡ç­”æ¡ˆï¼Œæå‡å¯è¯»æ€§
"""

    # ç”¨æˆ·é—®é¢˜æ¨¡æ¿
    USER_PROMPT_TEMPLATE = """ç”¨æˆ·é—®é¢˜ï¼š{question}

å‚è€ƒèµ„æ–™ï¼ˆæ¥è‡ªçŸ¥è¯†åº“ï¼‰ï¼š
{context}

è¯·åŸºäºä»¥ä¸Šå‚è€ƒèµ„æ–™å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœå‚è€ƒèµ„æ–™å·²ç»åŒ…å«å®Œæ•´ç­”æ¡ˆï¼Œè¯·ç›´æ¥ä½¿ç”¨ï¼›å¦‚æœéœ€è¦è¡¥å……è¯´æ˜ï¼Œè¯·åœ¨ä¿æŒå‡†ç¡®æ€§çš„å‰æä¸‹é€‚å½“å±•å¼€ã€‚
"""

    # æ— å‚è€ƒèµ„æ–™æ—¶çš„æ¨¡æ¿
    NO_CONTEXT_PROMPT = """ç”¨æˆ·é—®é¢˜ï¼š{question}

çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›´æ¥ç›¸å…³çš„å‚è€ƒèµ„æ–™ã€‚

è¯·åŸºäºä½ çš„ä¸“ä¸šçŸ¥è¯†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œå¹¶åœ¨å›ç­”å¼€å¤´è¯´æ˜"çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›´æ¥ç›¸å…³çš„å†…å®¹ï¼Œä»¥ä¸‹æ˜¯åŸºäºé€šç”¨çŸ¥è¯†çš„å›ç­”"ã€‚
"""

    @classmethod
    def format_context(cls, search_results, max_results=5):
        """
        æ ¼å¼åŒ–æ£€ç´¢ç»“æœä¸ºä¸Šä¸‹æ–‡
        :param search_results: æ£€ç´¢ç»“æœ
        :param max_results: æœ€å¤šä½¿ç”¨çš„ç»“æœæ•°é‡
        :return: æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        if not search_results or len(search_results) == 0:
            return None
        
        context_parts = []
        for idx, result in enumerate(search_results[:max_results], 1):
            subject = result.get('subject', 'N/A')
            question = result.get('question', 'N/A')
            answer = result.get('answer', 'N/A')
            score = result.get('score', 0)
            
            context_parts.append(f"""
ã€å‚è€ƒèµ„æ–™ {idx}ã€‘(ç›¸å…³æ€§å¾—åˆ†: {score:.4f})
å­¦ç§‘ï¼š{subject}
é—®é¢˜ï¼š{question}
ç­”æ¡ˆï¼š{answer}
""")
        
        return "\n".join(context_parts)
    
    @classmethod
    def create_messages(cls, question, context=None):
        """
        åˆ›å»ºå¯¹è¯æ¶ˆæ¯åˆ—è¡¨
        :param question: ç”¨æˆ·é—®é¢˜
        :param context: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
        :return: æ¶ˆæ¯åˆ—è¡¨
        """
        messages = [
            {"role": "system", "content": cls.SYSTEM_PROMPT}
        ]
        
        if context:
            user_content = cls.USER_PROMPT_TEMPLATE.format(
                question=question,
                context=context
            )
        else:
            user_content = cls.NO_CONTEXT_PROMPT.format(question=question)
        
        messages.append({"role": "user", "content": user_content})
        
        return messages


class RAGSystem:
    """æ··åˆæœç´¢ + é‡æ’RAGç³»ç»Ÿ"""
    
    def __init__(self, db_name='test1017', collection_name='jp_knowledge_qa'):
        """åˆå§‹åŒ–RAGç³»ç»Ÿ"""
        print("æ­£åœ¨åˆå§‹åŒ–RAGç³»ç»Ÿï¼ˆæ··åˆæœç´¢+é‡æ’ç‰ˆæœ¬ï¼‰...")
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        print("1. è¿æ¥å‘é‡æ•°æ®åº“...")
        self.db_name = db_name
        self.collection_name = collection_name
        self.client = MyMilvusClient(db_name=db_name)
        
        # åŠ è½½Embeddingæ¨¡å‹
        print("2. åŠ è½½Embeddingæ¨¡å‹...")
        self.model = SentenceTransformer("Qwen/Qwen3-Embedding-0.6B")
        
        # åŠ è½½é‡æ’æ¨¡å‹
        print("3. åŠ è½½é‡æ’æ¨¡å‹...")
        self.reranker = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True)
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        print("4. åˆå§‹åŒ–LLMå®¢æˆ·ç«¯...")
        self.llm_client = OpenAI(
            api_key=config.API_KEY,
            base_url=config.BASE_URL
        )
        self.llm_model = config.MODEL
        
        # åˆå§‹åŒ–BM25ï¼ˆç”¨äºç”Ÿæˆç¨€ç–å‘é‡ï¼‰
        self.bm25 = None
        self.question_list = None
        self._init_bm25()
        
        print("âœ“ RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼\n")
    
    def _init_bm25(self):
        """åˆå§‹åŒ–BM25æ¨¡å‹ï¼ˆä»æ•°æ®åº“åŠ è½½æ‰€æœ‰é—®é¢˜ï¼‰"""
        print("   æ­£åœ¨åˆå§‹åŒ–BM25æ¨¡å‹...")
        try:
            # ä»æ•°æ®åº“åŠ è½½æ‰€æœ‰é—®é¢˜
            from rank_bm25 import BM25Okapi
            all_docs = self.client.query(
                self.collection_name,
                filter_expr="id > 0",
                output_fields=["question"],
                limit=10000
            )
            
            self.question_list = [doc['question'] for doc in all_docs]
            tokenized_corpus = [list(jieba.cut(q)) for q in self.question_list]
            self.bm25 = BM25Okapi(tokenized_corpus)
            print(f"   BM25æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼ˆè¯­æ–™åº“å¤§å°: {len(self.question_list)}ï¼‰")
        except Exception as e:
            print(f"   BM25åˆå§‹åŒ–å¤±è´¥: {e}")
            self.bm25 = None
    
    def get_sparse_vector(self, text):
        """
        ç”ŸæˆBM25ç¨€ç–å‘é‡
        :param text: æŸ¥è¯¢æ–‡æœ¬
        :return: ç¨€ç–å‘é‡ï¼ˆå­—å…¸æ ¼å¼ï¼‰
        """
        if self.bm25 is None or self.question_list is None:
            return {}
        
        tokenized_query = list(jieba.cut(text))
        scores = self.bm25.get_scores(tokenized_query)
        
        # è½¬æ¢ä¸ºç¨€ç–å‘é‡æ ¼å¼
        sparse_vector = {}
        for idx, score in enumerate(scores):
            if score > 0:
                token = self.question_list[idx][:10]  # ä½¿ç”¨é—®é¢˜å‰10ä¸ªå­—ç¬¦ä½œä¸ºtoken
                sparse_vector[hash(token) % 100000] = float(score)
        
        return sparse_vector
    
    def hybrid_retrieve(self, query, top_k=40, subject_filter=None):
        """
        æ··åˆæœç´¢ï¼šç¨ å¯†å‘é‡ + BM25ç¨€ç–å‘é‡
        :param query: æŸ¥è¯¢é—®é¢˜
        :param top_k: è¿”å›top kä¸ªç»“æœï¼ˆé»˜è®¤40ï¼‰
        :param subject_filter: å­¦ç§‘è¿‡æ»¤
        :return: æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        # ç”Ÿæˆç¨ å¯†å‘é‡
        dense_vector = self.model.encode([query], prompt_name="query")[0].tolist()
        
        # ç”Ÿæˆç¨€ç–å‘é‡
        sparse_vector = self.get_sparse_vector(query)
        
        # æ„å»ºè¿‡æ»¤æ¡ä»¶
        filter_expr = None
        if subject_filter:
            filter_expr = f'subject == "{subject_filter}"'
        
        # æ‰§è¡Œæ··åˆæœç´¢
        try:
            search_results = self.client.hybrid_search(
                collection_name=self.collection_name,
                dense_vector=dense_vector,
                sparse_vector=sparse_vector,
                limit=top_k,
                output_fields=["id", "subject", "question", "answer"],
                filter=filter_expr
            )
            
            # å¤„ç†ç»“æœ
            results = []
            for hits in search_results:
                for hit in hits:
                    results.append({
                        'id': hit.get('id'),
                        'subject': hit.get('subject'),
                        'question': hit.get('question'),
                        'answer': hit.get('answer'),
                        'distance': hit.get('distance', 1.0)
                    })
            
            return results
        except Exception as e:
            # å¦‚æœæ··åˆæœç´¢å¤±è´¥ï¼Œé™çº§ä¸ºæ™®é€šå‘é‡æœç´¢
            print(f"æ··åˆæœç´¢å¤±è´¥ï¼Œé™çº§ä¸ºæ™®é€šæœç´¢: {e}")
            return self._fallback_search(query, top_k, subject_filter)
    
    def _fallback_search(self, query, top_k, subject_filter):
        """é™çº§æœç´¢æ–¹æ¡ˆï¼šä»…ä½¿ç”¨ç¨ å¯†å‘é‡"""
        dense_vector = self.model.encode([query], prompt_name="query")[0].tolist()
        filter_expr = f'subject == "{subject_filter}"' if subject_filter else None
        
        search_results = self.client.search(
            collection_name=self.collection_name,
            data=[dense_vector],
            limit=top_k,
            output_fields=["id", "subject", "question", "answer"],
            filter=filter_expr,
            anns_field="dense_vector"
        )
        
        results = []
        for hits in search_results:
            for hit in hits:
                results.append({
                    'id': hit.get('id'),
                    'subject': hit.get('subject'),
                    'question': hit.get('question'),
                    'answer': hit.get('answer'),
                    'distance': hit.get('distance', 1.0)
                })
        
        return results
    
    def rerank(self, query, candidates, top_k=5):
        """
        é‡æ’åºå€™é€‰ç»“æœ
        :param query: æŸ¥è¯¢é—®é¢˜
        :param candidates: å€™é€‰ç»“æœåˆ—è¡¨
        :param top_k: è¿”å›top kä¸ªç»“æœï¼ˆé»˜è®¤5ï¼‰
        :return: é‡æ’åçš„ç»“æœåˆ—è¡¨
        """
        if not candidates:
            return []
        
        # å‡†å¤‡é‡æ’è¾“å…¥
        pairs = [[query, candidate['question'] + ' ' + candidate['answer'][:200]] 
                 for candidate in candidates]
        
        # æ‰§è¡Œé‡æ’
        scores = self.reranker.compute_score(pairs)
        
        # å°†åˆ†æ•°æ·»åŠ åˆ°å€™é€‰ç»“æœä¸­
        for i, candidate in enumerate(candidates):
            candidate['score'] = scores[i] if isinstance(scores, list) else scores
        
        # æŒ‰åˆ†æ•°æ’åº
        reranked_results = sorted(candidates, key=lambda x: x['score'], reverse=True)
        
        return reranked_results[:top_k]
    
    def retrieve(self, query, subject_filter=None, hybrid_top_k=40, final_top_k=5):
        """
        å®Œæ•´æ£€ç´¢æµç¨‹ï¼šæ··åˆæœç´¢ + é‡æ’
        :param query: æŸ¥è¯¢é—®é¢˜
        :param subject_filter: å­¦ç§‘è¿‡æ»¤
        :param hybrid_top_k: æ··åˆæœç´¢è¿”å›çš„å€™é€‰æ•°é‡ï¼ˆé»˜è®¤40ï¼‰
        :param final_top_k: é‡æ’åæœ€ç»ˆè¿”å›çš„ç»“æœæ•°é‡ï¼ˆé»˜è®¤5ï¼‰
        :return: æœ€ç»ˆæ£€ç´¢ç»“æœåˆ—è¡¨
        """
        # 1. æ··åˆæœç´¢è·å–å€™é€‰ç»“æœ
        candidates = self.hybrid_retrieve(query, top_k=hybrid_top_k, subject_filter=subject_filter)
        
        if not candidates:
            return []
        
        # 2. é‡æ’åº
        final_results = self.rerank(query, candidates, top_k=final_top_k)
        
        return final_results
    
    def generate(self, messages, stream=False, temperature=0.7, max_tokens=2000):
        """
        ä½¿ç”¨LLMç”Ÿæˆå›ç­”
        :param messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
        :param stream: æ˜¯å¦æµå¼è¾“å‡º
        :param temperature: æ¸©åº¦å‚æ•°ï¼ˆ0-1ï¼Œè¶Šé«˜è¶Šéšæœºï¼‰
        :param max_tokens: æœ€å¤§tokenæ•°
        :return: ç”Ÿæˆçš„å›ç­”
        """
        try:
            response = self.llm_client.chat.completions.create(
                model=self.llm_model,
                messages=messages,
                stream=stream,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            if stream:
                return response
            else:
                # å…¼å®¹ä¸åŒAPIç«™çš„è¿”å›æ ¼å¼
                if isinstance(response, str):
                    return response
                elif hasattr(response, 'choices') and response.choices:
                    return response.choices[0].message.content
                else:
                    return str(response)
                
        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            return f"LLMè°ƒç”¨å¤±è´¥: {e}\nè¯¦ç»†é”™è¯¯:\n{error_detail}"
    
    def answer(self, question, subject_filter=None, stream=False, show_context=True, 
               temperature=0.7, hybrid_top_k=40, final_top_k=5):
        """
        å®Œæ•´çš„RAGé—®ç­”æµç¨‹ï¼ˆæ··åˆæœç´¢+é‡æ’ï¼‰
        :param question: ç”¨æˆ·é—®é¢˜
        :param subject_filter: å­¦ç§‘è¿‡æ»¤
        :param stream: æ˜¯å¦æµå¼è¾“å‡º
        :param show_context: æ˜¯å¦æ˜¾ç¤ºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
        :param temperature: LLMæ¸©åº¦å‚æ•°
        :param hybrid_top_k: æ··åˆæœç´¢å€™é€‰æ•°é‡
        :param final_top_k: æœ€ç»ˆè¿”å›ç»“æœæ•°é‡
        :return: å›ç­”å†…å®¹
        """
        print("=" * 80)
        print(f"é—®é¢˜ï¼š{question}")
        if subject_filter:
            print(f"é™å®šå­¦ç§‘ï¼š{subject_filter}")
        print("=" * 80)
        
        # 1. æ··åˆæ£€ç´¢ + é‡æ’
        print(f"\n[1/3] æ­£åœ¨æ£€ç´¢ç›¸å…³çŸ¥è¯†ï¼ˆæ··åˆæœç´¢Top{hybrid_top_k} -> é‡æ’Top{final_top_k}ï¼‰...")
        results = self.retrieve(
            query=question,
            subject_filter=subject_filter,
            hybrid_top_k=hybrid_top_k,
            final_top_k=final_top_k
        )
        
        if results:
            print(f"âœ“ æœ€ç»ˆè·å¾— {len(results)} æ¡é«˜è´¨é‡å‚è€ƒèµ„æ–™")
        else:
            print("âœ— æœªæ‰¾åˆ°ç›¸å…³å‚è€ƒèµ„æ–™ï¼Œå°†ä½¿ç”¨LLMé€šç”¨çŸ¥è¯†å›ç­”")
        
        # 2. æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
        context = PromptTemplate.format_context(results, max_results=final_top_k)
        
        # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
        if show_context and results:
            print("\n" + "-" * 80)
            print("æ£€ç´¢åˆ°çš„å‚è€ƒèµ„æ–™ï¼ˆé‡æ’åï¼‰ï¼š")
            print("-" * 80)
            for idx, result in enumerate(results[:final_top_k], 1):
                print(f"\nã€{idx}ã€‘ç›¸å…³æ€§å¾—åˆ†: {result.get('score', 0):.4f}")
                print(f"å­¦ç§‘ï¼š{result.get('subject', 'N/A')}")
                print(f"é—®é¢˜ï¼š{result.get('question', 'N/A')[:80]}...")
            print("-" * 80)
        
        # 3. ç”Ÿæˆå›ç­”
        print("\n[2/3] æ­£åœ¨ç”Ÿæˆå›ç­”...")
        messages = PromptTemplate.create_messages(question, context)
        
        print("\n[3/3] å›ç­”å†…å®¹ï¼š")
        print("=" * 80)
        
        if stream:
            # æµå¼è¾“å‡º
            response_stream = self.generate(messages, stream=True, temperature=temperature)
            full_response = ""
            try:
                if response_stream is None:
                    print("\né”™è¯¯ï¼šæœªè·å–åˆ°æµå¼å“åº”")
                    return None
                    
                for chunk in response_stream:
                    # å…¼å®¹ä¸åŒAPIç«™çš„æµå¼è¿”å›æ ¼å¼
                    if isinstance(chunk, str):
                        content = chunk
                        print(content, end='', flush=True)
                        full_response += content
                    elif hasattr(chunk, 'choices') and chunk.choices:
                        delta = chunk.choices[0].delta
                        if hasattr(delta, 'content') and delta.content:
                            content = delta.content
                            print(content, end='', flush=True)
                            full_response += content
                print()  # æ¢è¡Œ
            except Exception as e:
                import traceback
                error_detail = traceback.format_exc()
                print(f"\næµå¼è¾“å‡ºé”™è¯¯: {e}")
                print(f"è¯¦ç»†é”™è¯¯:\n{error_detail}")
                return None
            return full_response
        else:
            # éæµå¼è¾“å‡º
            answer = self.generate(messages, stream=False, temperature=temperature)
            print(answer)
            print("=" * 80)
            return answer
    
    def chat(self):
        """äº¤äº’å¼èŠå¤©æ¨¡å¼"""
        print("\n" + "=" * 80)
        print("RAG æ™ºèƒ½é—®ç­”ç³»ç»Ÿ - äº¤äº’æ¨¡å¼ï¼ˆæ··åˆæœç´¢+é‡æ’ï¼‰")
        print("=" * 80)
        print("\nä½¿ç”¨è¯´æ˜ï¼š")
        print("  - ç›´æ¥è¾“å…¥é—®é¢˜è¿›è¡Œæé—®")
        print("  - è¾“å…¥ '/python' æˆ– '/java' é™å®šå­¦ç§‘")
        print("  - è¾“å…¥ '/stream' åˆ‡æ¢æµå¼/éæµå¼è¾“å‡º")
        print("  - è¾“å…¥ '/context' åˆ‡æ¢æ˜¯å¦æ˜¾ç¤ºå‚è€ƒèµ„æ–™")
        print("  - è¾“å…¥ '/clear' æ¸…ç©ºå­¦ç§‘é™å®š")
        print("  - è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
        print("=" * 80 + "\n")
        
        subject_filter = None
        stream_mode = True
        show_context = True
        
        while True:
            try:
                user_input = input("\nğŸ’¬ æ‚¨çš„é—®é¢˜: ").strip()
                
                if not user_input:
                    continue
                
                # å¤„ç†å‘½ä»¤
                if user_input.lower() in ['quit', 'exit', 'q', 'é€€å‡º']:
                    print("\næ„Ÿè°¢ä½¿ç”¨ï¼å†è§ï¼ğŸ‘‹")
                    break
                
                if user_input.lower() == '/python':
                    subject_filter = "Pythonå­¦ç§‘"
                    print(f"âœ“ å·²é™å®šå­¦ç§‘ï¼š{subject_filter}")
                    continue
                
                if user_input.lower() == '/java':
                    subject_filter = "JAVAå­¦ç§‘"
                    print(f"âœ“ å·²é™å®šå­¦ç§‘ï¼š{subject_filter}")
                    continue
                
                if user_input.lower() == '/clear':
                    subject_filter = None
                    print("âœ“ å·²æ¸…é™¤å­¦ç§‘é™å®š")
                    continue
                
                if user_input.lower() == '/stream':
                    stream_mode = not stream_mode
                    print(f"âœ“ æµå¼è¾“å‡ºï¼š{'å¼€å¯' if stream_mode else 'å…³é—­'}")
                    continue
                
                if user_input.lower() == '/context':
                    show_context = not show_context
                    print(f"âœ“ æ˜¾ç¤ºå‚è€ƒèµ„æ–™ï¼š{'å¼€å¯' if show_context else 'å…³é—­'}")
                    continue
                
                # æ‰§è¡Œé—®ç­”
                self.answer(
                    question=user_input,
                    subject_filter=subject_filter,
                    stream=stream_mode,
                    show_context=show_context,
                    temperature=0.7,
                    hybrid_top_k=40,
                    final_top_k=5
                )
                
            except KeyboardInterrupt:
                print("\n\næ„Ÿè°¢ä½¿ç”¨ï¼å†è§ï¼ğŸ‘‹")
                break
            except Exception as e:
                print(f"\nâŒ é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 80)
    print("RAG æ™ºèƒ½é—®ç­”ç³»ç»Ÿï¼ˆæ··åˆæœç´¢+é‡æ’ç‰ˆæœ¬ï¼‰")
    print("=" * 80 + "\n")
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    rag = RAGSystem()
    
    # ç¤ºä¾‹é—®ç­”
    print("\n" + "=" * 80)
    print("ç¤ºä¾‹é—®ç­”")
    print("=" * 80 + "\n")
    
    # ç¤ºä¾‹1ï¼šPythonè£…é¥°å™¨
    rag.answer(
        question="å¦‚ä½•ä½¿ç”¨è£…é¥°å™¨è®¡ç®—å‡½æ•°è¿è¡Œæ—¶é—´ï¼Ÿ",
        stream=False,
        show_context=True,
        hybrid_top_k=40,
        final_top_k=5
    )
    
    print("\n" + "=" * 80 + "\n")
    
    # è¯¢é—®æ˜¯å¦è¿›å…¥äº¤äº’æ¨¡å¼
    choice = input("æ˜¯å¦è¿›å…¥äº¤äº’æ¨¡å¼ï¼Ÿ(y/n): ").strip().lower()
    if choice in ['y', 'yes', 'æ˜¯']:
        rag.chat()
    else:
        print("\næ„Ÿè°¢ä½¿ç”¨ï¼")


if __name__ == "__main__":
    main()
